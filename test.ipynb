{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b603983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f952a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f731e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e28a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/historical', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7afef2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for LSTM forecasting model.\"\"\"\n",
    "    sequence_length: int = 168  # 7 days * 24 hours (weekly patterns)\n",
    "    prediction_horizon: int = 24  # Predict 24 hours ahead\n",
    "    input_features: int = 10  # Number of input features\n",
    "    hidden_size: int = 128  # LSTM hidden size\n",
    "    num_layers: int = 3  # Number of LSTM layers\n",
    "    dropout: float = 0.2  # Dropout rate\n",
    "    attention_heads: int = 8  # Multi-head attention\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 100\n",
    "\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"Configuration for feature engineering pipeline.\"\"\"\n",
    "    # Technical indicators\n",
    "    rsi_period: int = 14\n",
    "    ma_short_period: int = 12\n",
    "    ma_long_period: int = 26\n",
    "    bb_period: int = 20\n",
    "    bb_std_dev: float = 2.0\n",
    "    \n",
    "    # Volatility features\n",
    "    volatility_windows: List[int] = field(default_factory=lambda: [6, 12, 24, 48])\n",
    "    \n",
    "    # Lag features\n",
    "    lag_periods: List[int] = field(default_factory=lambda: [1, 2, 3, 6, 12, 24])\n",
    "    \n",
    "    # Time features\n",
    "    include_time_features: bool = True\n",
    "    \n",
    "    # Scaling\n",
    "    scaler_type: str = \"robust\"  # \"standard\" or \"robust\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for model training.\"\"\"\n",
    "    model_config: ModelConfig = field(default_factory=ModelConfig)\n",
    "    feature_config: FeatureConfig = field(default_factory=FeatureConfig)\n",
    "    max_epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    patience: int = 10\n",
    "    validation_split: float = 0.2\n",
    "    device: str = \"auto\"  # \"auto\", \"cuda\", or \"cpu\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingResult:\n",
    "    \"\"\"Results from model training.\"\"\"\n",
    "    train_losses: List[float] = field(default_factory=list)\n",
    "    val_losses: List[float] = field(default_factory=list)\n",
    "    val_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    training_time: float = 0.0\n",
    "    best_epoch: int = 0\n",
    "    converged: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ed97f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection functions defined\n"
     ]
    }
   ],
   "source": [
    "from asyncio.log import logger\n",
    "\n",
    "\n",
    "class YFinanceDataCollector:\n",
    "    def __init__(self, data_dir: str = \"data/historical\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Yahoo Finance FX symbol mapping\n",
    "        self.fx_symbols = {\n",
    "            'USD/EUR': 'EURUSD=X',\n",
    "            'USD/GBP': 'GBPUSD=X', \n",
    "            'USD/JPY': 'USDJPY=X',\n",
    "            'EUR/GBP': 'EURGBP=X',\n",
    "            'EUR/JPY': 'EURJPY=X',\n",
    "            'GBP/JPY': 'GBPJPY=X',\n",
    "            'USD/CHF': 'USDCHF=X',\n",
    "            'EUR/CHF': 'EURCHF=X',\n",
    "            'GBP/CHF': 'GBPCHF=X',\n",
    "            'AUD/USD': 'AUDUSD=X',\n",
    "            'USD/CAD': 'USDCAD=X',\n",
    "            'NZD/USD': 'NZDUSD=X'\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Initialized YFinance collector with data directory: {self.data_dir}\")\n",
    "    \n",
    "    def download_historical_data(\n",
    "        self,\n",
    "        currency_pairs: List[str],\n",
    "        period: str = \"2y\",\n",
    "        interval: str = \"1h\",\n",
    "        save: bool = True\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        logger.info(f\"Downloading {len(currency_pairs)} currency pairs for period {period} with {interval} interval\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for pair in currency_pairs:\n",
    "            if pair not in self.fx_symbols:\n",
    "                logger.warning(f\"Currency pair {pair} not supported, skipping\")\n",
    "                continue\n",
    "            \n",
    "            yahoo_symbol = self.fx_symbols[pair]\n",
    "            logger.info(f\"Downloading {pair} ({yahoo_symbol})...\")\n",
    "            \n",
    "            try:\n",
    "                # Download data from Yahoo Finance\n",
    "                ticker = yf.Ticker(yahoo_symbol)\n",
    "                data = ticker.history(\n",
    "                    period=period,\n",
    "                    interval=interval,\n",
    "                    auto_adjust=True,\n",
    "                    prepost=True\n",
    "                )\n",
    "                \n",
    "                if data.empty:\n",
    "                    logger.error(f\"No data received for {pair}\")\n",
    "                    continue\n",
    "                \n",
    "                # Clean and prepare data\n",
    "                df = self._prepare_dataframe(data, pair)\n",
    "                results[pair] = df\n",
    "                \n",
    "                logger.info(f\"Downloaded {len(df)} records for {pair} from {df.index.min()} to {df.index.max()}\")\n",
    "                \n",
    "                # Save to disk if requested\n",
    "                if save:\n",
    "                    self._save_data(df, pair, period, interval)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download data for {pair}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Downloaded {len(results)} currency pairs successfully\")\n",
    "        return results\n",
    "    \n",
    "    def _prepare_dataframe(self, data: pd.DataFrame, currency_pair: str) -> pd.DataFrame:\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Rename columns to match our format\n",
    "        df = df.rename(columns={\n",
    "            'Open': 'open',\n",
    "            'High': 'high', \n",
    "            'Low': 'low',\n",
    "            'Close': 'close',\n",
    "            'Volume': 'volume'\n",
    "        })\n",
    "        \n",
    "        # Use close price as main rate\n",
    "        df['rate'] = df['close']\n",
    "        \n",
    "        # Add currency pair column\n",
    "        df['currency_pair'] = currency_pair\n",
    "        df['provider'] = 'YahooFinance'\n",
    "        \n",
    "        # Reset index to make timestamp a column\n",
    "        df = df.reset_index()\n",
    "        df = df.rename(columns={'Datetime': 'timestamp'})\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _save_data(self, df: pd.DataFrame, currency_pair: str, period: str, interval: str):\n",
    "        \"\"\"Save DataFrame to disk.\"\"\"\n",
    "        filename = f\"{currency_pair.replace('/', '_')}_{period}_{interval}.csv\"\n",
    "        filepath = self.data_dir / filename\n",
    "        \n",
    "        df.to_csv(filepath, index=False)\n",
    "        logger.info(f\"Saved {len(df)} records to {filepath}\")\n",
    "        \n",
    "        # Also save as pickle for faster loading\n",
    "        pickle_path = filepath.with_suffix('.pkl')\n",
    "        df.to_pickle(pickle_path)\n",
    "    \n",
    "    def load_historical_data(\n",
    "        self,\n",
    "        currency_pair: str,\n",
    "        period: str = \"2y\", \n",
    "        interval: str = \"1h\"\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load previously downloaded data from disk.\"\"\"\n",
    "        filename = f\"{currency_pair.replace('/', '_')}_{period}_{interval}.pkl\"\n",
    "        filepath = self.data_dir / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            logger.warning(f\"No saved data found for {currency_pair} at {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_pickle(filepath)\n",
    "            logger.info(f\"Loaded {len(df)} records for {currency_pair} from {filepath}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data for {currency_pair}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_available_data(self) -> List[Dict]:\n",
    "        \"\"\"Get list of available downloaded data files.\"\"\"\n",
    "        available = []\n",
    "        \n",
    "        for file_path in self.data_dir.glob(\"*.pkl\"):\n",
    "            try:\n",
    "                parts = file_path.stem.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    currency_pair = parts[0] + '/' + parts[1]\n",
    "                    period = parts[2]\n",
    "                    interval = parts[3] if len(parts) > 3 else '1h'\n",
    "                    \n",
    "                    # Get file info\n",
    "                    df = pd.read_pickle(file_path)\n",
    "                    \n",
    "                    available.append({\n",
    "                        'currency_pair': currency_pair,\n",
    "                        'period': period,\n",
    "                        'interval': interval,\n",
    "                        'records': len(df),\n",
    "                        'start_date': df['timestamp'].min(),\n",
    "                        'end_date': df['timestamp'].max(),\n",
    "                        'file_path': str(file_path)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not read file {file_path}: {e}\")\n",
    "        \n",
    "        return available\n",
    "    \n",
    "    def get_data_summary(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Get summary statistics for the dataset.\"\"\"\n",
    "        if df.empty:\n",
    "            return {\"error\": \"Empty dataset\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_records\": len(df),\n",
    "            \"currency_pairs\": df['currency_pair'].nunique(),\n",
    "            \"pairs_list\": sorted(df['currency_pair'].unique().tolist()),\n",
    "            \"date_range\": {\n",
    "                \"start\": df['timestamp'].min(),\n",
    "                \"end\": df['timestamp'].max(),\n",
    "                \"days\": (df['timestamp'].max() - df['timestamp'].min()).days\n",
    "            },\n",
    "            \"rate_statistics\": {\n",
    "                \"mean\": df['rate'].mean(),\n",
    "                \"std\": df['rate'].std(),\n",
    "                \"min\": df['rate'].min(),\n",
    "                \"max\": df['rate'].max(),\n",
    "                \"median\": df['rate'].median()\n",
    "            },\n",
    "            \"missing_values\": df.isnull().sum().sum(),\n",
    "            \"data_quality\": {\n",
    "                \"complete_days\": len(df.groupby(df['timestamp'].dt.date)),\n",
    "                \"avg_records_per_day\": len(df) / max(1, (df['timestamp'].max() - df['timestamp'].min()).days)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"Data collection functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1dbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering functions defined\n"
     ]
    }
   ],
   "source": [
    "class FeatureEngineering:\n",
    "    def __init__(self, config: FeatureConfig):\n",
    "        self.config = config\n",
    "        self.scaler = None\n",
    "        self.feature_names: List[str] = []\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create technical indicators from OHLC data.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # RSI (Relative Strength Index)\n",
    "        result['rsi'] = self._calculate_rsi(result['rate'], self.config.rsi_period)\n",
    "        \n",
    "        # Moving averages\n",
    "        result['ma_short'] = result['rate'].rolling(window=self.config.ma_short_period).mean()\n",
    "        result['ma_long'] = result['rate'].rolling(window=self.config.ma_long_period).mean()\n",
    "        result['ma_ratio'] = result['ma_short'] / result['ma_long']\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_mean = result['rate'].rolling(window=self.config.bb_period).mean()\n",
    "        bb_std = result['rate'].rolling(window=self.config.bb_period).std()\n",
    "        result['bb_upper'] = bb_mean + (bb_std * self.config.bb_std_dev)\n",
    "        result['bb_lower'] = bb_mean - (bb_std * self.config.bb_std_dev)\n",
    "        result['bb_position'] = (result['rate'] - result['bb_lower']) / (result['bb_upper'] - result['bb_lower'])\n",
    "        \n",
    "        # Price momentum\n",
    "        result['momentum_24h'] = result['rate'].pct_change(periods=24)\n",
    "        result['momentum_12h'] = result['rate'].pct_change(periods=12)\n",
    "        result['momentum_6h'] = result['rate'].pct_change(periods=6)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create volatility-based features.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Returns for volatility calculation\n",
    "        result['returns'] = result['rate'].pct_change()\n",
    "        \n",
    "        # Rolling volatilities\n",
    "        for window in self.config.volatility_windows:\n",
    "            col_name = f'volatility_{window}h'\n",
    "            result[col_name] = result['returns'].rolling(window=window).std()\n",
    "        \n",
    "        # Volatility of volatility (second-order)\n",
    "        if 'volatility_24h' in result.columns:\n",
    "            result['vol_of_vol'] = result['volatility_24h'].rolling(window=24).std()\n",
    "        \n",
    "        # Realized vs implied volatility proxy\n",
    "        if 'volatility_6h' in result.columns and 'volatility_24h' in result.columns:\n",
    "            result['vol_ratio'] = result['volatility_6h'] / (result['volatility_24h'] + 1e-8)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create time-based cyclical features.\"\"\"\n",
    "        if not self.config.include_time_features:\n",
    "            return df\n",
    "        \n",
    "        result = df.copy()\n",
    "        \n",
    "        # Ensure timestamp is datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(result['timestamp']):\n",
    "            result['timestamp'] = pd.to_datetime(result['timestamp'])\n",
    "        \n",
    "        # Hour of day (cyclical encoding)\n",
    "        result['hour_sin'] = np.sin(2 * np.pi * result['timestamp'].dt.hour / 24)\n",
    "        result['hour_cos'] = np.cos(2 * np.pi * result['timestamp'].dt.hour / 24)\n",
    "        \n",
    "        # Day of week (cyclical encoding)\n",
    "        result['dow_sin'] = np.sin(2 * np.pi * result['timestamp'].dt.dayofweek / 7)\n",
    "        result['dow_cos'] = np.cos(2 * np.pi * result['timestamp'].dt.dayofweek / 7)\n",
    "        \n",
    "        # Month of year (for seasonal patterns)\n",
    "        result['month_sin'] = np.sin(2 * np.pi * result['timestamp'].dt.month / 12)\n",
    "        result['month_cos'] = np.cos(2 * np.pi * result['timestamp'].dt.month / 12)\n",
    "        \n",
    "        # Market session indicators\n",
    "        result['is_market_hours'] = self._is_market_hours(result['timestamp'])\n",
    "        result['is_weekend'] = (result['timestamp'].dt.dayofweek >= 5).astype(int)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create lagged features and returns.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Lagged rates\n",
    "        for lag in self.config.lag_periods:\n",
    "            result[f'rate_lag_{lag}'] = result['rate'].shift(lag)\n",
    "        \n",
    "        # Lagged returns\n",
    "        returns = result['rate'].pct_change()\n",
    "        for lag in self.config.lag_periods:\n",
    "            result[f'returns_lag_{lag}'] = returns.shift(lag)\n",
    "        \n",
    "        # Lagged technical indicators\n",
    "        if 'rsi' in result.columns:\n",
    "            for lag in [1, 6, 12]:\n",
    "                result[f'rsi_lag_{lag}'] = result['rsi'].shift(lag)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Complete feature engineering pipeline.\"\"\"\n",
    "        logger.info(\"Starting feature engineering pipeline\")\n",
    "        \n",
    "        # Ensure data is sorted by timestamp\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Create all feature types\n",
    "        df = self.create_technical_indicators(df)\n",
    "        df = self.create_volatility_features(df)\n",
    "        df = self.create_time_features(df)\n",
    "        df = self.create_lag_features(df)\n",
    "        \n",
    "        # Remove rows with NaN values (from indicators and lags)\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna()\n",
    "        final_rows = len(df)\n",
    "        \n",
    "        logger.info(f\"Feature engineering complete: {initial_rows} -> {final_rows} rows\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_scaler(self, df: pd.DataFrame, feature_columns: List[str]) -> None:\n",
    "        \"\"\"Fit the scaler on training data.\"\"\"\n",
    "        if self.config.scaler_type == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        else:\n",
    "            self.scaler = RobustScaler()\n",
    "        \n",
    "        self.scaler.fit(df[feature_columns])\n",
    "        self.feature_names = feature_columns\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        logger.info(f\"Fitted {self.config.scaler_type} scaler on {len(feature_columns)} features\")\n",
    "    \n",
    "    def transform_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Transform features using fitted scaler.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Scaler not fitted. Call fit_scaler() first.\")\n",
    "        \n",
    "        return self.scaler.transform(df[self.feature_names])\n",
    "    \n",
    "    def fit_transform_features(self, df: pd.DataFrame, feature_columns: List[str]) -> np.ndarray:\n",
    "        \"\"\"Fit scaler and transform features in one step.\"\"\"\n",
    "        self.fit_scaler(df, feature_columns)\n",
    "        return self.transform_features(df)\n",
    "    \n",
    "    def get_feature_columns(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Get list of feature columns (excluding target and metadata).\"\"\"\n",
    "        exclude_cols = ['timestamp', 'rate', 'currency_pair', 'provider', 'bid', 'ask', 'volume', 'open', 'high', 'low', 'close']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        return feature_cols\n",
    "    \n",
    "    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:\n",
    "        \"\"\"Calculate Relative Strength Index.\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    \n",
    "    def _is_market_hours(self, timestamps: pd.Series) -> pd.Series:\n",
    "        \"\"\"Determine if timestamp falls within major market hours.\"\"\"\n",
    "        is_weekday = timestamps.dt.dayofweek < 5\n",
    "        is_business_hour = (timestamps.dt.hour >= 8) & (timestamps.dt.hour <= 18)\n",
    "        return (is_weekday & is_business_hour).astype(int)\n",
    "\n",
    "\n",
    "class SequenceGenerator:\n",
    "    \"\"\"Generate sequences for LSTM training from time series data.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length: int = 168, prediction_horizon: int = 24):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "    \n",
    "    def create_sequences(\n",
    "        self, \n",
    "        features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        stride: int = 1\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create input-output sequences for LSTM training.\"\"\"\n",
    "        n_samples, n_features = features.shape\n",
    "        \n",
    "        # Calculate number of sequences we can create\n",
    "        n_sequences = (n_samples - self.sequence_length - self.prediction_horizon + 1) // stride\n",
    "        \n",
    "        X = np.zeros((n_sequences, self.sequence_length, n_features))\n",
    "        y = np.zeros((n_sequences, self.prediction_horizon))\n",
    "        \n",
    "        for i in range(n_sequences):\n",
    "            start_idx = i * stride\n",
    "            end_idx = start_idx + self.sequence_length\n",
    "            target_start = end_idx\n",
    "            target_end = target_start + self.prediction_horizon\n",
    "            \n",
    "            X[i] = features[start_idx:end_idx]\n",
    "            y[i] = targets[target_start:target_end]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def create_single_sequence(self, features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create a single sequence for prediction (no target needed).\"\"\"\n",
    "        if len(features) < self.sequence_length:\n",
    "            raise ValueError(f\"Not enough data points. Need {self.sequence_length}, got {len(features)}\")\n",
    "        \n",
    "        # Take the last sequence_length rows\n",
    "        sequence = features[-self.sequence_length:]\n",
    "        return sequence.reshape(1, self.sequence_length, -1)\n",
    "\n",
    "print(\"Feature engineering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa3157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model architecture defined\n"
     ]
    }
   ],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_layers = config.num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config.input_features,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout if config.num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=config.hidden_size,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "        # Output layers\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Separate heads for mean and variance prediction\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_size // 2, config.prediction_horizon)\n",
    "        )\n",
    "        \n",
    "        self.variance_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size // 2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_size // 2, config.prediction_horizon),\n",
    "            nn.Softplus()  # Ensure positive variance\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using Xavier/Glorot initialization.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize LSTM hidden states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        lstm_out = self.layer_norm(lstm_out + attn_out)\n",
    "        \n",
    "        # Use the last timestep output\n",
    "        last_output = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Apply dropout\n",
    "        features = self.dropout(last_output)\n",
    "        \n",
    "        # Predict mean and variance\n",
    "        predicted_mean = self.mean_head(features)\n",
    "        predicted_variance = self.variance_head(features)\n",
    "        \n",
    "        return predicted_mean, predicted_variance\n",
    "    \n",
    "    def predict_with_uncertainty(self, x: torch.Tensor, num_samples: int = 100) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Make predictions with uncertainty quantification using Monte Carlo dropout.\"\"\"\n",
    "        self.train()  # Enable dropout for MC sampling\n",
    "        \n",
    "        predictions = []\n",
    "        variances = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                pred_mean, pred_var = self.forward(x)\n",
    "                predictions.append(pred_mean)\n",
    "                variances.append(pred_var)\n",
    "        \n",
    "        predictions = torch.stack(predictions)  # (num_samples, batch_size, prediction_horizon)\n",
    "        variances = torch.stack(variances)\n",
    "        \n",
    "        # Calculate uncertainties\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        epistemic_uncertainty = predictions.var(dim=0)  # Model uncertainty\n",
    "        aleatoric_uncertainty = variances.mean(dim=0)   # Data uncertainty\n",
    "        \n",
    "        self.eval()  # Return to eval mode\n",
    "        \n",
    "        return mean_prediction, epistemic_uncertainty, aleatoric_uncertainty\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to prevent overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 10, min_delta: float = 1e-6):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_true: torch.Tensor, y_pred_mean: torch.Tensor, y_pred_var: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Gaussian negative log-likelihood loss for uncertainty-aware training.\"\"\"\n",
    "    # Add small epsilon to prevent log(0)\n",
    "    epsilon = 1e-8\n",
    "    y_pred_var = y_pred_var + epsilon\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    nll = 0.5 * (torch.log(y_pred_var) + (y_true - y_pred_mean)**2 / y_pred_var)\n",
    "    return nll.mean()\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Utility class for model evaluation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "        \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
    "        # Handle multi-dimensional arrays by flattening if needed\n",
    "        if y_true.ndim > 1:\n",
    "            y_true = y_true.flatten()\n",
    "        if y_pred.ndim > 1:\n",
    "            y_pred = y_pred.flatten()\n",
    "        \n",
    "        # Basic error metrics\n",
    "        errors = y_true - y_pred\n",
    "        absolute_errors = np.abs(errors)\n",
    "        squared_errors = errors ** 2\n",
    "        \n",
    "        mse = np.mean(squared_errors)\n",
    "        mae = np.mean(absolute_errors)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Percentage-based metrics\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-8))) * 100\n",
    "        \n",
    "        # Mean Absolute Scaled Error (MASE) - scaled by naive forecast\n",
    "        naive_forecast_mae = np.mean(np.abs(np.diff(y_true)))\n",
    "        mase = mae / (naive_forecast_mae + 1e-8)\n",
    "        \n",
    "        # Directional accuracy (did we predict the right direction?)\n",
    "        if len(y_true) > 1:\n",
    "            y_true_diff = np.diff(y_true)\n",
    "            y_pred_diff = np.diff(y_pred)\n",
    "            directional_accuracy = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff)) * 100\n",
    "        else:\n",
    "            directional_accuracy = 0.0\n",
    "        \n",
    "        # R-squared\n",
    "        ss_res = np.sum(squared_errors)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        \n",
    "        # Additional metrics\n",
    "        median_ae = np.median(absolute_errors)\n",
    "        max_error = np.max(absolute_errors)\n",
    "        std_error = np.std(errors)\n",
    "        \n",
    "        # Symmetric metrics\n",
    "        smape = 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-8))\n",
    "        \n",
    "        return {\n",
    "            'mse': float(mse),\n",
    "            'mae': float(mae),\n",
    "            'rmse': float(rmse),\n",
    "            'mape': float(mape),\n",
    "            'smape': float(smape),\n",
    "            'mase': float(mase),\n",
    "            'r2': float(r2),\n",
    "            'directional_accuracy': float(directional_accuracy),\n",
    "            'median_absolute_error': float(median_ae),\n",
    "            'max_error': float(max_error),\n",
    "            'std_error': float(std_error),\n",
    "            'mean_error': float(np.mean(errors))\n",
    "        }\n",
    "\n",
    "print(\"LSTM model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc633ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Complete model training pipeline for FX forecasting.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = self._get_device(config.device)\n",
    "        self.model = None\n",
    "        self.feature_engineer = None\n",
    "        self.sequence_generator = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        \n",
    "        logger.info(f\"Initialized ModelTrainer on device: {self.device}\")\n",
    "    \n",
    "    def _get_device(self, device: str) -> torch.device:\n",
    "        \"\"\"Get appropriate device for training.\"\"\"\n",
    "        if device == \"auto\":\n",
    "            return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return torch.device(device)\n",
    "    \n",
    "    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare data for training.\"\"\"\n",
    "        logger.info(f\"Preparing data: {len(df)} samples\")\n",
    "        \n",
    "        # Feature engineering\n",
    "        self.feature_engineer = FeatureEngineering(self.config.feature_config)\n",
    "        df_features = self.feature_engineer.prepare_features(df)\n",
    "        \n",
    "        if df_features.empty:\n",
    "            raise ValueError(\"Feature engineering resulted in empty dataset\")\n",
    "        \n",
    "        # Get feature columns and fit scaler\n",
    "        feature_columns = self.feature_engineer.get_feature_columns(df_features)\n",
    "        logger.info(f\"Feature columns ({len(feature_columns)}): {feature_columns[:10]}...\")\n",
    "        \n",
    "        features_scaled = self.feature_engineer.fit_transform_features(df_features, feature_columns)\n",
    "        \n",
    "        # Update model config with actual number of features\n",
    "        self.config.model_config.input_features = len(feature_columns)\n",
    "        \n",
    "        # Extract targets\n",
    "        targets = df_features['rate'].values\n",
    "        \n",
    "        # Generate sequences\n",
    "        self.sequence_generator = SequenceGenerator(\n",
    "            self.config.model_config.sequence_length,\n",
    "            self.config.model_config.prediction_horizon\n",
    "        )\n",
    "        \n",
    "        X, y = self.sequence_generator.create_sequences(features_scaled, targets)\n",
    "        \n",
    "        logger.info(f\"Generated sequences - X: {X.shape}, y: {y.shape}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_model(self, X: np.ndarray, y: np.ndarray) -> TrainingResult:\n",
    "        \"\"\"Train the LSTM model.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(X) * (1 - self.config.validation_split))\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        logger.info(f\"Training split: {len(X_train)} train, {len(X_val)} validation\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train).to(self.device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).to(self.device)\n",
    "        X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
    "        y_val_tensor = torch.FloatTensor(y_val).to(self.device)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = LSTMForecaster(self.config.model_config).to(self.device)\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        early_stopping = EarlyStopping(patience=self.config.patience)\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            # Create batches\n",
    "            n_batches = len(X_train) // self.config.batch_size\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * self.config.batch_size\n",
    "                end_idx = min(start_idx + self.config.batch_size, len(X_train))\n",
    "                \n",
    "                batch_X = X_train_tensor[start_idx:end_idx]\n",
    "                batch_y = y_train_tensor[start_idx:end_idx]\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_mean, pred_var = self.model(batch_X)\n",
    "                \n",
    "                # Compute loss (using Gaussian NLL)\n",
    "                loss = gaussian_nll_loss(batch_y, pred_mean, pred_var)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / n_batches\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred_mean, pred_var = self.model(X_val_tensor)\n",
    "                val_loss = gaussian_nll_loss(y_val_tensor, pred_mean, pred_var).item()\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 10 == 0 or epoch == self.config.max_epochs - 1:\n",
    "                logger.info(f\"Epoch {epoch:3d}: Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping(val_loss):\n",
    "                logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred_mean, _ = self.model(X_val_tensor)\n",
    "            val_pred_np = val_pred_mean.cpu().numpy()\n",
    "            val_true_np = y_val\n",
    "            \n",
    "            val_metrics = ModelEvaluator.calculate_metrics(val_true_np, val_pred_np)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        result = TrainingResult(\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            val_metrics=val_metrics,\n",
    "            training_time=training_time,\n",
    "            best_epoch=best_epoch,\n",
    "            converged=not early_stopping.early_stop\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training completed in {training_time:.2f}s\")\n",
    "        logger.info(f\"Best validation MAPE: {val_metrics['mape']:.2f}%\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_model(self, path: str) -> None:\n",
    "        \"\"\"Save trained model to disk.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save\")\n",
    "        \n",
    "        save_dict = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'config': self.config,\n",
    "            'feature_engineer': self.feature_engineer,\n",
    "            'sequence_generator': self.sequence_generator,\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "        \n",
    "        logger.info(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path: str) -> None:\n",
    "        \"\"\"Load trained model from disk.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            save_dict = pickle.load(f)\n",
    "        \n",
    "        self.config = save_dict['config']\n",
    "        self.feature_engineer = save_dict['feature_engineer']\n",
    "        self.sequence_generator = save_dict['sequence_generator']\n",
    "        \n",
    "        # Create and load model\n",
    "        self.model = LSTMForecaster(self.config.model_config).to(self.device)\n",
    "        self.model.load_state_dict(save_dict['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2956de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Collection\n",
      "Currency Pairs: ['USD/EUR', 'USD/GBP', 'EUR/GBP']\n",
      "Period: 1y, Interval: 1h\n",
      "==================================================\n",
      "Found existing data:\n",
      "   USD/GBP: 6177 records (2024-08-23 to 2025-08-22)\n",
      "   EUR/GBP: 6178 records (2024-08-23 to 2025-08-22)\n",
      "   USD/EUR: 6177 records (2024-08-23 to 2025-08-22)\n",
      "Data collection complete! Loaded 3 currency pairs.\n"
     ]
    }
   ],
   "source": [
    "# Configuration for data collection\n",
    "CURRENCY_PAIRS = ['USD/EUR', 'USD/GBP', 'EUR/GBP']\n",
    "DATA_PERIOD = \"1y\"  # 1 year of historical data\n",
    "DATA_INTERVAL = \"1h\"  # Hourly data\n",
    "\n",
    "print(\"Starting Data Collection\")\n",
    "print(f\"Currency Pairs: {CURRENCY_PAIRS}\")\n",
    "print(f\"Period: {DATA_PERIOD}, Interval: {DATA_INTERVAL}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize data collector\n",
    "collector = YFinanceDataCollector()\n",
    "\n",
    "# Check for existing data first\n",
    "available_data = collector.get_available_data()\n",
    "if available_data:\n",
    "    print(\"Found existing data:\")\n",
    "    for data_info in available_data:\n",
    "        print(f\"   {data_info['currency_pair']}: {data_info['records']} records \"\n",
    "              f\"({data_info['start_date'].date()} to {data_info['end_date'].date()})\")\n",
    "    \n",
    "    # Load existing data\n",
    "    fx_data = {}\n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        df = collector.load_historical_data(pair, DATA_PERIOD, DATA_INTERVAL)\n",
    "        if df is not None:\n",
    "            fx_data[pair] = df\n",
    "else:\n",
    "    # Download fresh data\n",
    "    fx_data = collector.download_historical_data(\n",
    "        currency_pairs=CURRENCY_PAIRS,\n",
    "        period=DATA_PERIOD,\n",
    "        interval=DATA_INTERVAL,\n",
    "        save=True\n",
    "    )\n",
    "\n",
    "print(f\"Data collection complete! Loaded {len(fx_data)} currency pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b196980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "currency_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
