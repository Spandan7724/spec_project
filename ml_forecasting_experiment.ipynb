{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currency Assistant - ML Forecasting Engine Experimentation\n",
    "\n",
    "This notebook provides a complete, self-contained environment to experiment with the ML forecasting engine for foreign exchange rate prediction. All functions are defined explicitly in this notebook for easy experimentation.\n",
    "\n",
    "## Key Features\n",
    "- **Real Data Collection**: Download historical FX data from Yahoo Finance\n",
    "- **LSTM Model**: Advanced neural network with attention mechanism\n",
    "- **Uncertainty Quantification**: Monte Carlo dropout for prediction confidence\n",
    "- **Comprehensive Metrics**: MAE, MSE, RMSE, MAPE, R¬≤, and more\n",
    "- **Interactive Experimentation**: Easy parameter tuning and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Setup complete! Ready for ML experimentation.\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('data/historical', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for LSTM forecasting model.\"\"\"\n",
    "    sequence_length: int = 168  # 7 days * 24 hours (weekly patterns)\n",
    "    prediction_horizon: int = 24  # Predict 24 hours ahead\n",
    "    input_features: int = 10  # Number of input features\n",
    "    hidden_size: int = 128  # LSTM hidden size\n",
    "    num_layers: int = 3  # Number of LSTM layers\n",
    "    dropout: float = 0.2  # Dropout rate\n",
    "    attention_heads: int = 8  # Multi-head attention\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 100\n",
    "\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"Configuration for feature engineering pipeline.\"\"\"\n",
    "    # Technical indicators\n",
    "    rsi_period: int = 14\n",
    "    ma_short_period: int = 12\n",
    "    ma_long_period: int = 26\n",
    "    bb_period: int = 20\n",
    "    bb_std_dev: float = 2.0\n",
    "    \n",
    "    # Volatility features\n",
    "    volatility_windows: List[int] = field(default_factory=lambda: [6, 12, 24, 48])\n",
    "    \n",
    "    # Lag features\n",
    "    lag_periods: List[int] = field(default_factory=lambda: [1, 2, 3, 6, 12, 24])\n",
    "    \n",
    "    # Time features\n",
    "    include_time_features: bool = True\n",
    "    \n",
    "    # Scaling\n",
    "    scaler_type: str = \"robust\"  # \"standard\" or \"robust\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for model training.\"\"\"\n",
    "    model_config: ModelConfig = field(default_factory=ModelConfig)\n",
    "    feature_config: FeatureConfig = field(default_factory=FeatureConfig)\n",
    "    max_epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    patience: int = 10\n",
    "    validation_split: float = 0.2\n",
    "    device: str = \"auto\"  # \"auto\", \"cuda\", or \"cpu\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingResult:\n",
    "    \"\"\"Results from model training.\"\"\"\n",
    "    train_losses: List[float] = field(default_factory=list)\n",
    "    val_losses: List[float] = field(default_factory=list)\n",
    "    val_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    training_time: float = 0.0\n",
    "    best_epoch: int = 0\n",
    "    converged: bool = False\n",
    "\n",
    "print(\"‚úÖ Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YFinanceDataCollector:\n",
    "    \"\"\"\n",
    "    Collects historical FX data from Yahoo Finance for ML training.\n",
    "    \n",
    "    Yahoo Finance provides reliable historical data for major currency pairs\n",
    "    with good coverage and reasonable data quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"data/historical\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Yahoo Finance FX symbol mapping\n",
    "        self.fx_symbols = {\n",
    "            'USD/EUR': 'EURUSD=X',\n",
    "            'USD/GBP': 'GBPUSD=X', \n",
    "            'USD/JPY': 'USDJPY=X',\n",
    "            'EUR/GBP': 'EURGBP=X',\n",
    "            'EUR/JPY': 'EURJPY=X',\n",
    "            'GBP/JPY': 'GBPJPY=X',\n",
    "            'USD/CHF': 'USDCHF=X',\n",
    "            'EUR/CHF': 'EURCHF=X',\n",
    "            'GBP/CHF': 'GBPCHF=X',\n",
    "            'AUD/USD': 'AUDUSD=X',\n",
    "            'USD/CAD': 'USDCAD=X',\n",
    "            'NZD/USD': 'NZDUSD=X'\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Initialized YFinance collector with data directory: {self.data_dir}\")\n",
    "    \n",
    "    def download_historical_data(\n",
    "        self,\n",
    "        currency_pairs: List[str],\n",
    "        period: str = \"2y\",\n",
    "        interval: str = \"1h\",\n",
    "        save: bool = True\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Download historical FX data from Yahoo Finance.\n",
    "        \n",
    "        Args:\n",
    "            currency_pairs: List of currency pairs (e.g., ['USD/EUR', 'USD/GBP'])\n",
    "            period: Time period ('1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max')\n",
    "            interval: Data interval ('1m', '2m', '5m', '15m', '30m', '60m', '90m', '1h', '1d', '5d', '1wk', '1mo', '3mo')\n",
    "            save: Whether to save data to disk\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping currency pairs to DataFrames\n",
    "        \"\"\"\n",
    "        logger.info(f\"Downloading {len(currency_pairs)} currency pairs for period {period} with {interval} interval\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for pair in currency_pairs:\n",
    "            if pair not in self.fx_symbols:\n",
    "                logger.warning(f\"Currency pair {pair} not supported, skipping\")\n",
    "                continue\n",
    "            \n",
    "            yahoo_symbol = self.fx_symbols[pair]\n",
    "            logger.info(f\"Downloading {pair} ({yahoo_symbol})...\")\n",
    "            \n",
    "            try:\n",
    "                # Download data from Yahoo Finance\n",
    "                ticker = yf.Ticker(yahoo_symbol)\n",
    "                data = ticker.history(\n",
    "                    period=period,\n",
    "                    interval=interval,\n",
    "                    auto_adjust=True,\n",
    "                    prepost=True\n",
    "                )\n",
    "                \n",
    "                if data.empty:\n",
    "                    logger.error(f\"No data received for {pair}\")\n",
    "                    continue\n",
    "                \n",
    "                # Clean and prepare data\n",
    "                df = self._prepare_dataframe(data, pair)\n",
    "                results[pair] = df\n",
    "                \n",
    "                logger.info(f\"Downloaded {len(df)} records for {pair} from {df.index.min()} to {df.index.max()}\")\n",
    "                \n",
    "                # Save to disk if requested\n",
    "                if save:\n",
    "                    self._save_data(df, pair, period, interval)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download data for {pair}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Downloaded {len(results)} currency pairs successfully\")\n",
    "        return results\n",
    "    \n",
    "    def _prepare_dataframe(self, data: pd.DataFrame, currency_pair: str) -> pd.DataFrame:\n",
    "        \"\"\"Prepare and clean Yahoo Finance data.\"\"\"\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Rename columns to match our format\n",
    "        df = df.rename(columns={\n",
    "            'Open': 'open',\n",
    "            'High': 'high', \n",
    "            'Low': 'low',\n",
    "            'Close': 'close',\n",
    "            'Volume': 'volume'\n",
    "        })\n",
    "        \n",
    "        # Use close price as main rate\n",
    "        df['rate'] = df['close']\n",
    "        \n",
    "        # Add currency pair column\n",
    "        df['currency_pair'] = currency_pair\n",
    "        df['provider'] = 'YahooFinance'\n",
    "        \n",
    "        # Reset index to make timestamp a column\n",
    "        df = df.reset_index()\n",
    "        df = df.rename(columns={'Datetime': 'timestamp'})\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _save_data(self, df: pd.DataFrame, currency_pair: str, period: str, interval: str):\n",
    "        \"\"\"Save DataFrame to disk.\"\"\"\n",
    "        filename = f\"{currency_pair.replace('/', '_')}_{period}_{interval}.csv\"\n",
    "        filepath = self.data_dir / filename\n",
    "        \n",
    "        df.to_csv(filepath, index=False)\n",
    "        logger.info(f\"Saved {len(df)} records to {filepath}\")\n",
    "        \n",
    "        # Also save as pickle for faster loading\n",
    "        pickle_path = filepath.with_suffix('.pkl')\n",
    "        df.to_pickle(pickle_path)\n",
    "    \n",
    "    def load_historical_data(\n",
    "        self,\n",
    "        currency_pair: str,\n",
    "        period: str = \"2y\", \n",
    "        interval: str = \"1h\"\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load previously downloaded data from disk.\"\"\"\n",
    "        filename = f\"{currency_pair.replace('/', '_')}_{period}_{interval}.pkl\"\n",
    "        filepath = self.data_dir / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            logger.warning(f\"No saved data found for {currency_pair} at {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_pickle(filepath)\n",
    "            logger.info(f\"Loaded {len(df)} records for {currency_pair} from {filepath}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data for {currency_pair}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_available_data(self) -> List[Dict]:\n",
    "        \"\"\"Get list of available downloaded data files.\"\"\"\n",
    "        available = []\n",
    "        \n",
    "        for file_path in self.data_dir.glob(\"*.pkl\"):\n",
    "            try:\n",
    "                parts = file_path.stem.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    currency_pair = parts[0] + '/' + parts[1]\n",
    "                    period = parts[2]\n",
    "                    interval = parts[3] if len(parts) > 3 else '1h'\n",
    "                    \n",
    "                    # Get file info\n",
    "                    df = pd.read_pickle(file_path)\n",
    "                    \n",
    "                    available.append({\n",
    "                        'currency_pair': currency_pair,\n",
    "                        'period': period,\n",
    "                        'interval': interval,\n",
    "                        'records': len(df),\n",
    "                        'start_date': df['timestamp'].min(),\n",
    "                        'end_date': df['timestamp'].max(),\n",
    "                        'file_path': str(file_path)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not read file {file_path}: {e}\")\n",
    "        \n",
    "        return available\n",
    "    \n",
    "    def get_data_summary(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Get summary statistics for the dataset.\"\"\"\n",
    "        if df.empty:\n",
    "            return {\"error\": \"Empty dataset\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_records\": len(df),\n",
    "            \"currency_pairs\": df['currency_pair'].nunique(),\n",
    "            \"pairs_list\": sorted(df['currency_pair'].unique().tolist()),\n",
    "            \"date_range\": {\n",
    "                \"start\": df['timestamp'].min(),\n",
    "                \"end\": df['timestamp'].max(),\n",
    "                \"days\": (df['timestamp'].max() - df['timestamp'].min()).days\n",
    "            },\n",
    "            \"rate_statistics\": {\n",
    "                \"mean\": df['rate'].mean(),\n",
    "                \"std\": df['rate'].std(),\n",
    "                \"min\": df['rate'].min(),\n",
    "                \"max\": df['rate'].max(),\n",
    "                \"median\": df['rate'].median()\n",
    "            },\n",
    "            \"missing_values\": df.isnull().sum().sum(),\n",
    "            \"data_quality\": {\n",
    "                \"complete_days\": len(df.groupby(df['timestamp'].dt.date)),\n",
    "                \"avg_records_per_day\": len(df) / max(1, (df['timestamp'].max() - df['timestamp'].min()).days)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"‚úÖ Data collection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    \"\"\"\n",
    "    Feature engineering pipeline for FX time series data.\n",
    "    \n",
    "    Transforms raw exchange rate data into ML-ready features including:\n",
    "    - Technical indicators (RSI, Moving Averages, Bollinger Bands)\n",
    "    - Time-based features (hour, day of week, etc.)\n",
    "    - Lag variables and returns\n",
    "    - Volatility measures\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: FeatureConfig):\n",
    "        self.config = config\n",
    "        self.scaler = None\n",
    "        self.feature_names: List[str] = []\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create technical indicators from OHLC data.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # RSI (Relative Strength Index)\n",
    "        result['rsi'] = self._calculate_rsi(result['rate'], self.config.rsi_period)\n",
    "        \n",
    "        # Moving averages\n",
    "        result['ma_short'] = result['rate'].rolling(window=self.config.ma_short_period).mean()\n",
    "        result['ma_long'] = result['rate'].rolling(window=self.config.ma_long_period).mean()\n",
    "        result['ma_ratio'] = result['ma_short'] / result['ma_long']\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_mean = result['rate'].rolling(window=self.config.bb_period).mean()\n",
    "        bb_std = result['rate'].rolling(window=self.config.bb_period).std()\n",
    "        result['bb_upper'] = bb_mean + (bb_std * self.config.bb_std_dev)\n",
    "        result['bb_lower'] = bb_mean - (bb_std * self.config.bb_std_dev)\n",
    "        result['bb_position'] = (result['rate'] - result['bb_lower']) / (result['bb_upper'] - result['bb_lower'])\n",
    "        \n",
    "        # Price momentum\n",
    "        result['momentum_24h'] = result['rate'].pct_change(periods=24)\n",
    "        result['momentum_12h'] = result['rate'].pct_change(periods=12)\n",
    "        result['momentum_6h'] = result['rate'].pct_change(periods=6)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create volatility-based features.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Returns for volatility calculation\n",
    "        result['returns'] = result['rate'].pct_change()\n",
    "        \n",
    "        # Rolling volatilities\n",
    "        for window in self.config.volatility_windows:\n",
    "            col_name = f'volatility_{window}h'\n",
    "            result[col_name] = result['returns'].rolling(window=window).std()\n",
    "        \n",
    "        # Volatility of volatility (second-order)\n",
    "        if 'volatility_24h' in result.columns:\n",
    "            result['vol_of_vol'] = result['volatility_24h'].rolling(window=24).std()\n",
    "        \n",
    "        # Realized vs implied volatility proxy\n",
    "        if 'volatility_6h' in result.columns and 'volatility_24h' in result.columns:\n",
    "            result['vol_ratio'] = result['volatility_6h'] / (result['volatility_24h'] + 1e-8)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create time-based cyclical features.\"\"\"\n",
    "        if not self.config.include_time_features:\n",
    "            return df\n",
    "        \n",
    "        result = df.copy()\n",
    "        \n",
    "        # Ensure timestamp is datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(result['timestamp']):\n",
    "            result['timestamp'] = pd.to_datetime(result['timestamp'])\n",
    "        \n",
    "        # Hour of day (cyclical encoding)\n",
    "        result['hour_sin'] = np.sin(2 * np.pi * result['timestamp'].dt.hour / 24)\n",
    "        result['hour_cos'] = np.cos(2 * np.pi * result['timestamp'].dt.hour / 24)\n",
    "        \n",
    "        # Day of week (cyclical encoding)\n",
    "        result['dow_sin'] = np.sin(2 * np.pi * result['timestamp'].dt.dayofweek / 7)\n",
    "        result['dow_cos'] = np.cos(2 * np.pi * result['timestamp'].dt.dayofweek / 7)\n",
    "        \n",
    "        # Month of year (for seasonal patterns)\n",
    "        result['month_sin'] = np.sin(2 * np.pi * result['timestamp'].dt.month / 12)\n",
    "        result['month_cos'] = np.cos(2 * np.pi * result['timestamp'].dt.month / 12)\n",
    "        \n",
    "        # Market session indicators\n",
    "        result['is_market_hours'] = self._is_market_hours(result['timestamp'])\n",
    "        result['is_weekend'] = (result['timestamp'].dt.dayofweek >= 5).astype(int)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create lagged features and returns.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Lagged rates\n",
    "        for lag in self.config.lag_periods:\n",
    "            result[f'rate_lag_{lag}'] = result['rate'].shift(lag)\n",
    "        \n",
    "        # Lagged returns\n",
    "        returns = result['rate'].pct_change()\n",
    "        for lag in self.config.lag_periods:\n",
    "            result[f'returns_lag_{lag}'] = returns.shift(lag)\n",
    "        \n",
    "        # Lagged technical indicators\n",
    "        if 'rsi' in result.columns:\n",
    "            for lag in [1, 6, 12]:\n",
    "                result[f'rsi_lag_{lag}'] = result['rsi'].shift(lag)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Complete feature engineering pipeline.\"\"\"\n",
    "        logger.info(\"Starting feature engineering pipeline\")\n",
    "        \n",
    "        # Ensure data is sorted by timestamp\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Create all feature types\n",
    "        df = self.create_technical_indicators(df)\n",
    "        df = self.create_volatility_features(df)\n",
    "        df = self.create_time_features(df)\n",
    "        df = self.create_lag_features(df)\n",
    "        \n",
    "        # Remove rows with NaN values (from indicators and lags)\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna()\n",
    "        final_rows = len(df)\n",
    "        \n",
    "        logger.info(f\"Feature engineering complete: {initial_rows} -> {final_rows} rows\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_scaler(self, df: pd.DataFrame, feature_columns: List[str]) -> None:\n",
    "        \"\"\"Fit the scaler on training data.\"\"\"\n",
    "        if self.config.scaler_type == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        else:\n",
    "            self.scaler = RobustScaler()\n",
    "        \n",
    "        self.scaler.fit(df[feature_columns])\n",
    "        self.feature_names = feature_columns\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        logger.info(f\"Fitted {self.config.scaler_type} scaler on {len(feature_columns)} features\")\n",
    "    \n",
    "    def transform_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Transform features using fitted scaler.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Scaler not fitted. Call fit_scaler() first.\")\n",
    "        \n",
    "        return self.scaler.transform(df[self.feature_names])\n",
    "    \n",
    "    def fit_transform_features(self, df: pd.DataFrame, feature_columns: List[str]) -> np.ndarray:\n",
    "        \"\"\"Fit scaler and transform features in one step.\"\"\"\n",
    "        self.fit_scaler(df, feature_columns)\n",
    "        return self.transform_features(df)\n",
    "    \n",
    "    def get_feature_columns(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Get list of feature columns (excluding target and metadata).\"\"\"\n",
    "        exclude_cols = ['timestamp', 'rate', 'currency_pair', 'provider', 'bid', 'ask', 'volume', 'open', 'high', 'low', 'close']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        return feature_cols\n",
    "    \n",
    "    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:\n",
    "        \"\"\"Calculate Relative Strength Index.\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    \n",
    "    def _is_market_hours(self, timestamps: pd.Series) -> pd.Series:\n",
    "        \"\"\"Determine if timestamp falls within major market hours.\"\"\"\n",
    "        is_weekday = timestamps.dt.dayofweek < 5\n",
    "        is_business_hour = (timestamps.dt.hour >= 8) & (timestamps.dt.hour <= 18)\n",
    "        return (is_weekday & is_business_hour).astype(int)\n",
    "\n",
    "\n",
    "class SequenceGenerator:\n",
    "    \"\"\"Generate sequences for LSTM training from time series data.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length: int = 168, prediction_horizon: int = 24):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "    \n",
    "    def create_sequences(\n",
    "        self, \n",
    "        features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        stride: int = 1\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create input-output sequences for LSTM training.\"\"\"\n",
    "        n_samples, n_features = features.shape\n",
    "        \n",
    "        # Calculate number of sequences we can create\n",
    "        n_sequences = (n_samples - self.sequence_length - self.prediction_horizon + 1) // stride\n",
    "        \n",
    "        X = np.zeros((n_sequences, self.sequence_length, n_features))\n",
    "        y = np.zeros((n_sequences, self.prediction_horizon))\n",
    "        \n",
    "        for i in range(n_sequences):\n",
    "            start_idx = i * stride\n",
    "            end_idx = start_idx + self.sequence_length\n",
    "            target_start = end_idx\n",
    "            target_end = target_start + self.prediction_horizon\n",
    "            \n",
    "            X[i] = features[start_idx:end_idx]\n",
    "            y[i] = targets[target_start:target_end]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def create_single_sequence(self, features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create a single sequence for prediction (no target needed).\"\"\"\n",
    "        if len(features) < self.sequence_length:\n",
    "            raise ValueError(f\"Not enough data points. Need {self.sequence_length}, got {len(features)}\")\n",
    "        \n",
    "        # Take the last sequence_length rows\n",
    "        sequence = features[-self.sequence_length:]\n",
    "        return sequence.reshape(1, self.sequence_length, -1)\n",
    "\n",
    "print(\"‚úÖ Feature engineering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based forecasting model with attention mechanism.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-layer LSTM for sequence modeling\n",
    "    - Multi-head attention for important pattern focus\n",
    "    - Uncertainty quantification (mean + variance prediction)\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_layers = config.num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config.input_features,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout if config.num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=config.hidden_size,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "        # Output layers\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Separate heads for mean and variance prediction\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_size // 2, config.prediction_horizon)\n",
    "        )\n",
    "        \n",
    "        self.variance_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size // 2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_size // 2, config.prediction_horizon),\n",
    "            nn.Softplus()  # Ensure positive variance\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using Xavier/Glorot initialization.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize LSTM hidden states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        lstm_out = self.layer_norm(lstm_out + attn_out)\n",
    "        \n",
    "        # Use the last timestep output\n",
    "        last_output = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Apply dropout\n",
    "        features = self.dropout(last_output)\n",
    "        \n",
    "        # Predict mean and variance\n",
    "        predicted_mean = self.mean_head(features)\n",
    "        predicted_variance = self.variance_head(features)\n",
    "        \n",
    "        return predicted_mean, predicted_variance\n",
    "    \n",
    "    def predict_with_uncertainty(self, x: torch.Tensor, num_samples: int = 100) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Make predictions with uncertainty quantification using Monte Carlo dropout.\"\"\"\n",
    "        self.train()  # Enable dropout for MC sampling\n",
    "        \n",
    "        predictions = []\n",
    "        variances = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                pred_mean, pred_var = self.forward(x)\n",
    "                predictions.append(pred_mean)\n",
    "                variances.append(pred_var)\n",
    "        \n",
    "        predictions = torch.stack(predictions)  # (num_samples, batch_size, prediction_horizon)\n",
    "        variances = torch.stack(variances)\n",
    "        \n",
    "        # Calculate uncertainties\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        epistemic_uncertainty = predictions.var(dim=0)  # Model uncertainty\n",
    "        aleatoric_uncertainty = variances.mean(dim=0)   # Data uncertainty\n",
    "        \n",
    "        self.eval()  # Return to eval mode\n",
    "        \n",
    "        return mean_prediction, epistemic_uncertainty, aleatoric_uncertainty\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to prevent overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 10, min_delta: float = 1e-6):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_true: torch.Tensor, y_pred_mean: torch.Tensor, y_pred_var: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Gaussian negative log-likelihood loss for uncertainty-aware training.\"\"\"\n",
    "    # Add small epsilon to prevent log(0)\n",
    "    epsilon = 1e-8\n",
    "    y_pred_var = y_pred_var + epsilon\n",
    "    \n",
    "    # Compute negative log-likelihood\n",
    "    nll = 0.5 * (torch.log(y_pred_var) + (y_true - y_pred_mean)**2 / y_pred_var)\n",
    "    return nll.mean()\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Utility class for model evaluation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "        \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
    "        # Handle multi-dimensional arrays by flattening if needed\n",
    "        if y_true.ndim > 1:\n",
    "            y_true = y_true.flatten()\n",
    "        if y_pred.ndim > 1:\n",
    "            y_pred = y_pred.flatten()\n",
    "        \n",
    "        # Basic error metrics\n",
    "        errors = y_true - y_pred\n",
    "        absolute_errors = np.abs(errors)\n",
    "        squared_errors = errors ** 2\n",
    "        \n",
    "        mse = np.mean(squared_errors)\n",
    "        mae = np.mean(absolute_errors)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Percentage-based metrics\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-8))) * 100\n",
    "        \n",
    "        # Mean Absolute Scaled Error (MASE) - scaled by naive forecast\n",
    "        naive_forecast_mae = np.mean(np.abs(np.diff(y_true)))\n",
    "        mase = mae / (naive_forecast_mae + 1e-8)\n",
    "        \n",
    "        # Directional accuracy (did we predict the right direction?)\n",
    "        if len(y_true) > 1:\n",
    "            y_true_diff = np.diff(y_true)\n",
    "            y_pred_diff = np.diff(y_pred)\n",
    "            directional_accuracy = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff)) * 100\n",
    "        else:\n",
    "            directional_accuracy = 0.0\n",
    "        \n",
    "        # R-squared\n",
    "        ss_res = np.sum(squared_errors)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        \n",
    "        # Additional metrics\n",
    "        median_ae = np.median(absolute_errors)\n",
    "        max_error = np.max(absolute_errors)\n",
    "        std_error = np.std(errors)\n",
    "        \n",
    "        # Symmetric metrics\n",
    "        smape = 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-8))\n",
    "        \n",
    "        return {\n",
    "            'mse': float(mse),\n",
    "            'mae': float(mae),\n",
    "            'rmse': float(rmse),\n",
    "            'mape': float(mape),\n",
    "            'smape': float(smape),\n",
    "            'mase': float(mase),\n",
    "            'r2': float(r2),\n",
    "            'directional_accuracy': float(directional_accuracy),\n",
    "            'median_absolute_error': float(median_ae),\n",
    "            'max_error': float(max_error),\n",
    "            'std_error': float(std_error),\n",
    "            'mean_error': float(np.mean(errors))\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ LSTM model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Complete model training pipeline for FX forecasting.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = self._get_device(config.device)\n",
    "        self.model = None\n",
    "        self.feature_engineer = None\n",
    "        self.sequence_generator = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        \n",
    "        logger.info(f\"Initialized ModelTrainer on device: {self.device}\")\n",
    "    \n",
    "    def _get_device(self, device: str) -> torch.device:\n",
    "        \"\"\"Get appropriate device for training.\"\"\"\n",
    "        if device == \"auto\":\n",
    "            return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return torch.device(device)\n",
    "    \n",
    "    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare data for training.\"\"\"\n",
    "        logger.info(f\"Preparing data: {len(df)} samples\")\n",
    "        \n",
    "        # Feature engineering\n",
    "        self.feature_engineer = FeatureEngineering(self.config.feature_config)\n",
    "        df_features = self.feature_engineer.prepare_features(df)\n",
    "        \n",
    "        if df_features.empty:\n",
    "            raise ValueError(\"Feature engineering resulted in empty dataset\")\n",
    "        \n",
    "        # Get feature columns and fit scaler\n",
    "        feature_columns = self.feature_engineer.get_feature_columns(df_features)\n",
    "        logger.info(f\"Feature columns ({len(feature_columns)}): {feature_columns[:10]}...\")\n",
    "        \n",
    "        features_scaled = self.feature_engineer.fit_transform_features(df_features, feature_columns)\n",
    "        \n",
    "        # Update model config with actual number of features\n",
    "        self.config.model_config.input_features = len(feature_columns)\n",
    "        \n",
    "        # Extract targets\n",
    "        targets = df_features['rate'].values\n",
    "        \n",
    "        # Generate sequences\n",
    "        self.sequence_generator = SequenceGenerator(\n",
    "            self.config.model_config.sequence_length,\n",
    "            self.config.model_config.prediction_horizon\n",
    "        )\n",
    "        \n",
    "        X, y = self.sequence_generator.create_sequences(features_scaled, targets)\n",
    "        \n",
    "        logger.info(f\"Generated sequences - X: {X.shape}, y: {y.shape}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_model(self, X: np.ndarray, y: np.ndarray) -> TrainingResult:\n",
    "        \"\"\"Train the LSTM model.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(X) * (1 - self.config.validation_split))\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        logger.info(f\"Training split: {len(X_train)} train, {len(X_val)} validation\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train).to(self.device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).to(self.device)\n",
    "        X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
    "        y_val_tensor = torch.FloatTensor(y_val).to(self.device)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = LSTMForecaster(self.config.model_config).to(self.device)\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        early_stopping = EarlyStopping(patience=self.config.patience)\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            # Create batches\n",
    "            n_batches = len(X_train) // self.config.batch_size\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * self.config.batch_size\n",
    "                end_idx = min(start_idx + self.config.batch_size, len(X_train))\n",
    "                \n",
    "                batch_X = X_train_tensor[start_idx:end_idx]\n",
    "                batch_y = y_train_tensor[start_idx:end_idx]\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_mean, pred_var = self.model(batch_X)\n",
    "                \n",
    "                # Compute loss (using Gaussian NLL)\n",
    "                loss = gaussian_nll_loss(batch_y, pred_mean, pred_var)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / n_batches\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred_mean, pred_var = self.model(X_val_tensor)\n",
    "                val_loss = gaussian_nll_loss(y_val_tensor, pred_mean, pred_var).item()\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 10 == 0 or epoch == self.config.max_epochs - 1:\n",
    "                logger.info(f\"Epoch {epoch:3d}: Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping(val_loss):\n",
    "                logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred_mean, _ = self.model(X_val_tensor)\n",
    "            val_pred_np = val_pred_mean.cpu().numpy()\n",
    "            val_true_np = y_val\n",
    "            \n",
    "            val_metrics = ModelEvaluator.calculate_metrics(val_true_np, val_pred_np)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        result = TrainingResult(\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            val_metrics=val_metrics,\n",
    "            training_time=training_time,\n",
    "            best_epoch=best_epoch,\n",
    "            converged=not early_stopping.early_stop\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training completed in {training_time:.2f}s\")\n",
    "        logger.info(f\"Best validation MAPE: {val_metrics['mape']:.2f}%\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_model(self, path: str) -> None:\n",
    "        \"\"\"Save trained model to disk.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save\")\n",
    "        \n",
    "        save_dict = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'config': self.config,\n",
    "            'feature_engineer': self.feature_engineer,\n",
    "            'sequence_generator': self.sequence_generator,\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "        \n",
    "        logger.info(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path: str) -> None:\n",
    "        \"\"\"Load trained model from disk.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            save_dict = pickle.load(f)\n",
    "        \n",
    "        self.config = save_dict['config']\n",
    "        self.feature_engineer = save_dict['feature_engineer']\n",
    "        self.sequence_generator = save_dict['sequence_generator']\n",
    "        \n",
    "        # Create and load model\n",
    "        self.model = LSTMForecaster(self.config.model_config).to(self.device)\n",
    "        self.model.load_state_dict(save_dict['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection and Exploration\n",
    "\n",
    "Let's start by downloading historical FX data and exploring its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for data collection\n",
    "CURRENCY_PAIRS = ['USD/EUR', 'USD/GBP', 'EUR/GBP']\n",
    "DATA_PERIOD = \"1y\"  # 1 year of historical data\n",
    "DATA_INTERVAL = \"1h\"  # Hourly data\n",
    "\n",
    "print(\"üìä Starting Data Collection\")\n",
    "print(f\"Currency Pairs: {CURRENCY_PAIRS}\")\n",
    "print(f\"Period: {DATA_PERIOD}, Interval: {DATA_INTERVAL}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize data collector\n",
    "collector = YFinanceDataCollector()\n",
    "\n",
    "# Check for existing data first\n",
    "available_data = collector.get_available_data()\n",
    "if available_data:\n",
    "    print(\"üìÅ Found existing data:\")\n",
    "    for data_info in available_data:\n",
    "        print(f\"   {data_info['currency_pair']}: {data_info['records']} records \"\n",
    "              f\"({data_info['start_date'].date()} to {data_info['end_date'].date()})\")\n",
    "    \n",
    "    # Load existing data\n",
    "    fx_data = {}\n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        df = collector.load_historical_data(pair, DATA_PERIOD, DATA_INTERVAL)\n",
    "        if df is not None:\n",
    "            fx_data[pair] = df\n",
    "else:\n",
    "    # Download fresh data\n",
    "    fx_data = collector.download_historical_data(\n",
    "        currency_pairs=CURRENCY_PAIRS,\n",
    "        period=DATA_PERIOD,\n",
    "        interval=DATA_INTERVAL,\n",
    "        save=True\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Data collection complete! Loaded {len(fx_data)} currency pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and visualization\n",
    "fig, axes = plt.subplots(len(fx_data), 1, figsize=(15, 4 * len(fx_data)))\n",
    "if len(fx_data) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "summaries = {}\n",
    "for i, (pair, df) in enumerate(fx_data.items()):\n",
    "    # Get summary statistics\n",
    "    summary = collector.get_data_summary(df)\n",
    "    summaries[pair] = summary\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nüìà {pair} Summary:\")\n",
    "    print(f\"   Records: {summary['total_records']:,}\")\n",
    "    print(f\"   Date Range: {summary['date_range']['start']} to {summary['date_range']['end']}\")\n",
    "    print(f\"   Duration: {summary['date_range']['days']} days\")\n",
    "    print(f\"   Rate Range: {summary['rate_statistics']['min']:.6f} - {summary['rate_statistics']['max']:.6f}\")\n",
    "    print(f\"   Mean: {summary['rate_statistics']['mean']:.6f} ¬± {summary['rate_statistics']['std']:.6f}\")\n",
    "    print(f\"   Missing Values: {summary['missing_values']}\")\n",
    "    print(f\"   Avg Records/Day: {summary['data_quality']['avg_records_per_day']:.1f}\")\n",
    "    \n",
    "    # Plot time series\n",
    "    axes[i].plot(df['timestamp'], df['rate'], linewidth=0.8, alpha=0.8)\n",
    "    axes[i].set_title(f'{pair} Exchange Rate Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Exchange Rate')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics text box\n",
    "    stats_text = f\"Records: {summary['total_records']:,}\\nMean: {summary['rate_statistics']['mean']:.6f}\\nStd: {summary['rate_statistics']['std']:.6f}\"\n",
    "    axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                verticalalignment='top', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration and Training\n",
    "\n",
    "Configure the LSTM model and training parameters. You can easily modify these to experiment with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration - Experiment with these parameters!\n",
    "model_config = ModelConfig(\n",
    "    sequence_length=168,  # 1 week of hourly data (7 * 24)\n",
    "    prediction_horizon=24,  # Predict 24 hours ahead\n",
    "    hidden_size=64,  # LSTM hidden size\n",
    "    num_layers=2,  # Number of LSTM layers\n",
    "    dropout=0.2,  # Dropout rate for regularization\n",
    "    attention_heads=8  # Multi-head attention\n",
    ")\n",
    "\n",
    "# Feature Engineering Configuration\n",
    "feature_config = FeatureConfig(\n",
    "    rsi_period=14,  # RSI period\n",
    "    ma_short_period=12,  # Short moving average\n",
    "    ma_long_period=26,  # Long moving average\n",
    "    bb_period=20,  # Bollinger Bands period\n",
    "    volatility_windows=[6, 12, 24, 48],  # Volatility calculation windows\n",
    "    lag_periods=[1, 2, 3, 6, 12, 24]  # Lagged features\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "training_config = TrainingConfig(\n",
    "    model_config=model_config,\n",
    "    feature_config=feature_config,\n",
    "    max_epochs=50,  # Maximum training epochs\n",
    "    batch_size=32,  # Batch size\n",
    "    learning_rate=0.001,  # Learning rate\n",
    "    patience=10,  # Early stopping patience\n",
    "    validation_split=0.2  # Validation split ratio\n",
    ")\n",
    "\n",
    "print(\"üèóÔ∏è  Model Configuration:\")\n",
    "print(f\"   Sequence Length: {model_config.sequence_length} hours\")\n",
    "print(f\"   Prediction Horizon: {model_config.prediction_horizon} hours\")\n",
    "print(f\"   Hidden Size: {model_config.hidden_size}\")\n",
    "print(f\"   Layers: {model_config.num_layers}\")\n",
    "print(f\"   Dropout: {model_config.dropout}\")\n",
    "print(f\"   Attention Heads: {model_config.attention_heads}\")\n",
    "print(f\"   Max Epochs: {training_config.max_epochs}\")\n",
    "print(f\"   Batch Size: {training_config.batch_size}\")\n",
    "print(f\"   Learning Rate: {training_config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Models for Each Currency Pair\n",
    "\n",
    "Now let's train LSTM models for each currency pair and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(df: pd.DataFrame, currency_pair: str, config: TrainingConfig):\n",
    "    \"\"\"\n",
    "    Train a single LSTM model for a currency pair.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß† Training ML Model for {currency_pair}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"üìä Training data: {len(df)} records\")\n",
    "    print(f\"   Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"   Duration: {(df['timestamp'].max() - df['timestamp'].min()).days} days\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ModelTrainer(config)\n",
    "    \n",
    "    print(\"üîÑ Preparing training data...\")\n",
    "    X, y = trainer.prepare_data(df)\n",
    "    \n",
    "    print(f\"   Training sequences: {X.shape[0]}\")\n",
    "    print(f\"   Input shape: {X.shape}\")\n",
    "    print(f\"   Target shape: {y.shape}\")\n",
    "    print(f\"   Features per timestep: {X.shape[2]}\")\n",
    "    \n",
    "    if X.shape[0] < 100:\n",
    "        print(\"‚ö†Ô∏è  Warning: Very few training sequences. Consider more data or shorter sequence length.\")\n",
    "    \n",
    "    print(\"\\nüöÄ Starting model training...\")\n",
    "    training_result = trainer.train_model(X, y)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed!\")\n",
    "    print(f\"   Training time: {training_result.training_time:.2f} seconds\")\n",
    "    print(f\"   Best epoch: {training_result.best_epoch}\")\n",
    "    print(f\"   Converged: {training_result.converged}\")\n",
    "    print(f\"   Final train loss: {training_result.train_losses[-1]:.6f}\")\n",
    "    print(f\"   Final val loss: {training_result.val_losses[-1]:.6f}\")\n",
    "    \n",
    "    return trainer, training_result\n",
    "\n",
    "# Train models for each currency pair\n",
    "trained_models = {}\n",
    "training_results = {}\n",
    "\n",
    "for pair, df in fx_data.items():\n",
    "    if len(df) < 1000:  # Need sufficient data\n",
    "        print(f\"‚ö†Ô∏è  Skipping {pair}: insufficient data ({len(df)} records)\")\n",
    "        continue\n",
    "    \n",
    "    trainer, result = train_single_model(df, pair, training_config)\n",
    "    \n",
    "    if trainer and result:\n",
    "        trained_models[pair] = trainer\n",
    "        training_results[pair] = result\n",
    "        \n",
    "        # Save trained model\n",
    "        model_path = f\"models/{pair.replace('/', '_')}_experiment.pkl\"\n",
    "        trainer.save_model(model_path)\n",
    "        print(f\"üíæ Model saved to: {model_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {pair} model training failed\")\n",
    "\n",
    "print(f\"\\nüéâ Training Summary: {len(trained_models)}/{len(fx_data)} models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Analysis\n",
    "\n",
    "Let's analyze the performance of our trained models with comprehensive metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics and visualization\n",
    "if trained_models:\n",
    "    # Create performance comparison\n",
    "    performance_data = []\n",
    "    \n",
    "    for pair, result in training_results.items():\n",
    "        metrics = result.val_metrics\n",
    "        performance_data.append({\n",
    "            'Currency Pair': pair,\n",
    "            'MSE': metrics['mse'],\n",
    "            'MAE': metrics['mae'],\n",
    "            'RMSE': metrics['rmse'],\n",
    "            'MAPE (%)': metrics['mape'],\n",
    "            'R¬≤': metrics['r2'],\n",
    "            'Directional Accuracy (%)': metrics['directional_accuracy'],\n",
    "            'Training Time (s)': result.training_time\n",
    "        })\n",
    "        \n",
    "        # Print detailed metrics\n",
    "        print(f\"\\nüìä {pair} Performance Metrics:\")\n",
    "        print(f\"   MSE (Mean Squared Error): {metrics['mse']:.8f}\")\n",
    "        print(f\"   MAE (Mean Absolute Error): {metrics['mae']:.6f}\")\n",
    "        print(f\"   RMSE (Root Mean Squared Error): {metrics['rmse']:.6f}\")\n",
    "        print(f\"   MAPE (Mean Absolute Percentage Error): {metrics['mape']:.2f}%\")\n",
    "        print(f\"   SMAPE (Symmetric MAPE): {metrics.get('smape', 'N/A'):.2f}%\")\n",
    "        print(f\"   MASE (Mean Absolute Scaled Error): {metrics.get('mase', 'N/A'):.4f}\")\n",
    "        print(f\"   R¬≤ (Coefficient of Determination): {metrics['r2']:.4f}\")\n",
    "        print(f\"   Directional Accuracy: {metrics['directional_accuracy']:.2f}%\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    print(\"\\nüìã Performance Comparison:\")\n",
    "    print(performance_df.round(6))\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(2, len(trained_models), figsize=(5 * len(trained_models), 10))\n",
    "    if len(trained_models) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, (pair, result) in enumerate(training_results.items()):\n",
    "        # Training and validation loss\n",
    "        axes[0, i].plot(result.train_losses, label='Training Loss', alpha=0.8)\n",
    "        axes[0, i].plot(result.val_losses, label='Validation Loss', alpha=0.8)\n",
    "        axes[0, i].axvline(x=result.best_epoch, color='red', linestyle='--', alpha=0.5, label='Best Epoch')\n",
    "        axes[0, i].set_title(f'{pair} - Training Curves')\n",
    "        axes[0, i].set_xlabel('Epoch')\n",
    "        axes[0, i].set_ylabel('Loss')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Metrics bar plot\n",
    "        metrics_names = ['MAPE (%)', 'R¬≤', 'Dir. Acc. (%)']\n",
    "        metrics_values = [\n",
    "            result.val_metrics['mape'],\n",
    "            result.val_metrics['r2'] * 100,  # Scale R¬≤ for visibility\n",
    "            result.val_metrics['directional_accuracy']\n",
    "        ]\n",
    "        \n",
    "        bars = axes[1, i].bar(metrics_names, metrics_values, alpha=0.7)\n",
    "        axes[1, i].set_title(f'{pair} - Key Metrics')\n",
    "        axes[1, i].set_ylabel('Value')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, metrics_values):\n",
    "            height = bar.get_height()\n",
    "            axes[1, i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                           f'{value:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make Predictions with Uncertainty\n",
    "\n",
    "Test the models by making predictions on recent data with uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_with_uncertainty(trainer, df: pd.DataFrame, currency_pair: str, num_samples: int = 100):\n",
    "    \"\"\"\n",
    "    Make predictions with uncertainty quantification.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÆ Testing Predictions for {currency_pair}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Use last 7 days of data for prediction testing\n",
    "    test_data = df.tail(7 * 24).copy()  # Last 7 days\n",
    "    \n",
    "    if len(test_data) < trainer.config.model_config.sequence_length:\n",
    "        print(\"‚ö†Ô∏è  Insufficient test data\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features for the test data\n",
    "    df_features = trainer.feature_engineer.prepare_features(test_data)\n",
    "    feature_columns = trainer.feature_engineer.get_feature_columns(df_features)\n",
    "    features_scaled = trainer.feature_engineer.transform_features(df_features)\n",
    "    \n",
    "    # Create sequence for prediction\n",
    "    sequence = trainer.sequence_generator.create_single_sequence(features_scaled)\n",
    "    \n",
    "    # Make prediction\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence_tensor = torch.FloatTensor(sequence).to(trainer.device)\n",
    "        \n",
    "        # Get standard prediction\n",
    "        pred_mean, pred_var = trainer.model(sequence_tensor)\n",
    "        \n",
    "        # Get prediction with uncertainty\n",
    "        pred_mean_mc, epistemic_unc, aleatoric_unc = trainer.model.predict_with_uncertainty(\n",
    "            sequence_tensor, num_samples=num_samples\n",
    "        )\n",
    "    \n",
    "    # Convert to numpy\n",
    "    predictions = pred_mean.cpu().numpy().flatten()\n",
    "    predictions_mc = pred_mean_mc.cpu().numpy().flatten()\n",
    "    epistemic_uncertainty = epistemic_unc.cpu().numpy().flatten()\n",
    "    aleatoric_uncertainty = aleatoric_unc.cpu().numpy().flatten()\n",
    "    \n",
    "    # Get current and recent rates\n",
    "    current_rate = test_data['rate'].iloc[-1]\n",
    "    recent_rates = test_data['rate'].tail(24).values  # Last 24 hours\n",
    "    \n",
    "    print(f\"üìà Current Rate: {current_rate:.6f}\")\n",
    "    print(f\"üìä Recent 24h Range: {recent_rates.min():.6f} - {recent_rates.max():.6f}\")\n",
    "    print(f\"üìä Recent 24h Change: {((current_rate - recent_rates[0]) / recent_rates[0] * 100):+.2f}%\")\n",
    "    \n",
    "    # Predictions at different horizons\n",
    "    horizons = [0, 5, 11, 23]  # 1h, 6h, 12h, 24h\n",
    "    horizon_names = ['1-hour', '6-hour', '12-hour', '24-hour']\n",
    "    \n",
    "    print(f\"\\nüîÆ Predictions (next 24 hours):\")\n",
    "    prediction_data = []\n",
    "    \n",
    "    for h, name in zip(horizons, horizon_names):\n",
    "        if h < len(predictions):\n",
    "            total_unc = np.sqrt(epistemic_uncertainty[h] + aleatoric_uncertainty[h])\n",
    "            change = ((predictions[h] - current_rate) / current_rate) * 100\n",
    "            \n",
    "            print(f\"   {name} ahead: {predictions[h]:.6f} ¬±{total_unc:.6f} ({change:+.3f}%)\")\n",
    "            \n",
    "            prediction_data.append({\n",
    "                'horizon': name,\n",
    "                'prediction': predictions[h],\n",
    "                'uncertainty': total_unc,\n",
    "                'change_pct': change\n",
    "            })\n",
    "    \n",
    "    # Uncertainty analysis\n",
    "    avg_epistemic = np.mean(epistemic_uncertainty[:24])\n",
    "    avg_aleatoric = np.mean(aleatoric_uncertainty[:24])\n",
    "    total_uncertainty = np.sqrt(avg_epistemic + avg_aleatoric)\n",
    "    \n",
    "    print(f\"\\nüéØ Uncertainty Analysis:\")\n",
    "    print(f\"   Model Uncertainty (Epistemic): {avg_epistemic:.6f}\")\n",
    "    print(f\"   Data Uncertainty (Aleatoric): {avg_aleatoric:.6f}\")\n",
    "    print(f\"   Total Uncertainty: {total_uncertainty:.6f}\")\n",
    "    \n",
    "    # Confidence assessment\n",
    "    confidence = max(0.0, min(1.0, 1.0 - total_uncertainty))\n",
    "    print(f\"   Model Confidence: {confidence:.2f} ({confidence*100:.0f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'uncertainty_epistemic': epistemic_uncertainty,\n",
    "        'uncertainty_aleatoric': aleatoric_uncertainty,\n",
    "        'current_rate': current_rate,\n",
    "        'prediction_data': prediction_data\n",
    "    }\n",
    "\n",
    "# Make predictions for each trained model\n",
    "prediction_results = {}\n",
    "\n",
    "for pair, trainer in trained_models.items():\n",
    "    df = fx_data[pair]\n",
    "    result = make_predictions_with_uncertainty(trainer, df, pair, num_samples=100)\n",
    "    if result:\n",
    "        prediction_results[pair] = result\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions completed for {len(prediction_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Visualization\n",
    "\n",
    "Visualize the predictions with uncertainty bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with uncertainty\n",
    "if prediction_results:\n",
    "    fig, axes = plt.subplots(len(prediction_results), 1, figsize=(15, 5 * len(prediction_results)))\n",
    "    if len(prediction_results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (pair, result) in enumerate(prediction_results.items()):\n",
    "        # Get recent data for context\n",
    "        recent_data = fx_data[pair].tail(48)  # Last 48 hours\n",
    "        \n",
    "        # Plot historical data\n",
    "        axes[i].plot(range(len(recent_data)), recent_data['rate'].values, \n",
    "                    label='Historical', color='blue', alpha=0.7, linewidth=1.5)\n",
    "        \n",
    "        # Plot predictions\n",
    "        pred_start = len(recent_data)\n",
    "        pred_hours = np.arange(pred_start, pred_start + len(result['predictions']))\n",
    "        \n",
    "        axes[i].plot(pred_hours, result['predictions'], \n",
    "                    label='Predictions', color='red', alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Plot uncertainty bands\n",
    "        total_uncertainty = np.sqrt(result['uncertainty_epistemic'] + result['uncertainty_aleatoric'])\n",
    "        \n",
    "        axes[i].fill_between(pred_hours,\n",
    "                            result['predictions'] - 1.96 * total_uncertainty,\n",
    "                            result['predictions'] + 1.96 * total_uncertainty,\n",
    "                            alpha=0.3, color='red', label='95% Confidence')\n",
    "        \n",
    "        # Add current rate line\n",
    "        axes[i].axhline(y=result['current_rate'], color='green', linestyle='--', \n",
    "                       alpha=0.7, label=f'Current Rate: {result[\"current_rate\"]:.6f}')\n",
    "        \n",
    "        axes[i].axvline(x=pred_start, color='black', linestyle=':', alpha=0.5, label='Prediction Start')\n",
    "        \n",
    "        axes[i].set_title(f'{pair} - 24-hour Predictions with Uncertainty', fontweight='bold')\n",
    "        axes[i].set_xlabel('Hours')\n",
    "        axes[i].set_ylabel('Exchange Rate')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add prediction summary as text\n",
    "        summary_text = f\"24h Prediction: {result['predictions'][23]:.6f}\\n\"\n",
    "        summary_text += f\"Expected Change: {((result['predictions'][23] - result['current_rate']) / result['current_rate'] * 100):+.2f}%\"\n",
    "        \n",
    "        axes[i].text(0.02, 0.98, summary_text, transform=axes[i].transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8),\n",
    "                    verticalalignment='top', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No prediction results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Summary\n",
    "\n",
    "Compare all models and summarize the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "if trained_models and prediction_results:\n",
    "    print(\"üéâ EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for pair in trained_models.keys():\n",
    "        training_result = training_results[pair]\n",
    "        prediction_result = prediction_results[pair]\n",
    "        \n",
    "        # Get 24-hour prediction\n",
    "        pred_24h = prediction_result['predictions'][23] if len(prediction_result['predictions']) > 23 else prediction_result['predictions'][-1]\n",
    "        change_24h = ((pred_24h - prediction_result['current_rate']) / prediction_result['current_rate']) * 100\n",
    "        \n",
    "        total_unc_24h = np.sqrt(\n",
    "            prediction_result['uncertainty_epistemic'][23] + prediction_result['uncertainty_aleatoric'][23]\n",
    "        ) if len(prediction_result['predictions']) > 23 else 0\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Currency Pair': pair,\n",
    "            'MAPE (%)': f\"{training_result.val_metrics['mape']:.2f}\",\n",
    "            'R¬≤': f\"{training_result.val_metrics['r2']:.4f}\",\n",
    "            'Dir. Accuracy (%)': f\"{training_result.val_metrics['directional_accuracy']:.1f}\",\n",
    "            'Current Rate': f\"{prediction_result['current_rate']:.6f}\",\n",
    "            '24h Prediction': f\"{pred_24h:.6f}\",\n",
    "            'Expected Change (%)': f\"{change_24h:+.2f}\",\n",
    "            'Uncertainty': f\"¬±{total_unc_24h:.6f}\",\n",
    "            'Training Time (s)': f\"{training_result.training_time:.1f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Performing Model (by MAPE):\")\n",
    "    best_pair = min(training_results.keys(), \n",
    "                   key=lambda k: training_results[k].val_metrics['mape'])\n",
    "    best_mape = training_results[best_pair].val_metrics['mape']\n",
    "    print(f\"   {best_pair}: {best_mape:.2f}% MAPE\")\n",
    "    \n",
    "    print(f\"\\nüìä Average Performance:\")\n",
    "    avg_mape = np.mean([result.val_metrics['mape'] for result in training_results.values()])\n",
    "    avg_r2 = np.mean([result.val_metrics['r2'] for result in training_results.values()])\n",
    "    avg_dir_acc = np.mean([result.val_metrics['directional_accuracy'] for result in training_results.values()])\n",
    "    \n",
    "    print(f\"   MAPE: {avg_mape:.2f}%\")\n",
    "    print(f\"   R¬≤: {avg_r2:.4f}\")\n",
    "    print(f\"   Directional Accuracy: {avg_dir_acc:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüíæ Trained models saved in 'models/' directory\")\n",
    "    print(\"üîó Ready for integration with Decision Engine\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Experiment incomplete - no models trained or predictions made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment with Different Configurations\n",
    "\n",
    "This section allows you to quickly experiment with different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick experimentation function\n",
    "def quick_experiment(currency_pair: str, \n",
    "                    sequence_length: int = 168,\n",
    "                    hidden_size: int = 64,\n",
    "                    num_layers: int = 2,\n",
    "                    max_epochs: int = 30):\n",
    "    \"\"\"\n",
    "    Quick experiment with custom parameters on a single currency pair.\n",
    "    \"\"\"\n",
    "    if currency_pair not in fx_data:\n",
    "        print(f\"‚ùå Currency pair {currency_pair} not available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üß™ Quick Experiment: {currency_pair}\")\n",
    "    print(f\"   Sequence Length: {sequence_length}\")\n",
    "    print(f\"   Hidden Size: {hidden_size}\")\n",
    "    print(f\"   Layers: {num_layers}\")\n",
    "    print(f\"   Max Epochs: {max_epochs}\")\n",
    "    \n",
    "    # Custom configuration\n",
    "    custom_model_config = ModelConfig(\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_horizon=24,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=0.2\n",
    "    )\n",
    "    \n",
    "    custom_training_config = TrainingConfig(\n",
    "        model_config=custom_model_config,\n",
    "        feature_config=feature_config,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "        patience=5,  # Shorter patience for quick experiments\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    df = fx_data[currency_pair]\n",
    "    trainer, result = train_single_model(df, currency_pair, custom_training_config)\n",
    "    \n",
    "    if trainer and result:\n",
    "        print(f\"\\nüìä Quick Results:\")\n",
    "        print(f\"   MAPE: {result.val_metrics['mape']:.2f}%\")\n",
    "        print(f\"   R¬≤: {result.val_metrics['r2']:.4f}\")\n",
    "        print(f\"   Training Time: {result.training_time:.1f}s\")\n",
    "        \n",
    "        # Quick prediction\n",
    "        pred_result = make_predictions_with_uncertainty(trainer, df, currency_pair, num_samples=50)\n",
    "        return trainer, result, pred_result\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Example: Experiment with different configurations\n",
    "print(\"üî¨ Try different configurations by running:\")\n",
    "print(\"quick_experiment('USD/EUR', sequence_length=96, hidden_size=32, max_epochs=20)\")\n",
    "print(\"quick_experiment('USD/GBP', sequence_length=240, hidden_size=128, num_layers=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Persistence and Loading\n",
    "\n",
    "Functions for saving and loading trained models for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence utilities\n",
    "def save_experiment_results(filename: str = \"experiment_results.pkl\"):\n",
    "    \"\"\"\n",
    "    Save all experiment results for later analysis.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'training_results': training_results,\n",
    "        'prediction_results': prediction_results,\n",
    "        'model_config': model_config,\n",
    "        'feature_config': feature_config,\n",
    "        'training_config': training_config,\n",
    "        'fx_data_summary': {pair: collector.get_data_summary(df) for pair, df in fx_data.items()}\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"üíæ Experiment results saved to {filename}\")\n",
    "\n",
    "def load_trained_model(model_path: str, config: TrainingConfig):\n",
    "    \"\"\"\n",
    "    Load a previously trained model for predictions.\n",
    "    \"\"\"\n",
    "    trainer = ModelTrainer(config)\n",
    "    trainer.load_model(model_path)\n",
    "    print(f\"üìÇ Model loaded from {model_path}\")\n",
    "    return trainer\n",
    "\n",
    "# List available saved models\n",
    "model_files = [f for f in os.listdir('models/') if f.endswith('.pkl')] if os.path.exists('models/') else []\n",
    "print(f\"üíæ Available saved models ({len(model_files)}):\")\n",
    "for model_file in model_files:\n",
    "    print(f\"   - {model_file}\")\n",
    "\n",
    "# Save experiment results\n",
    "if 'trained_models' in globals() and trained_models:\n",
    "    save_experiment_results(\"latest_experiment_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Production Integration\n",
    "\n",
    "Ideas for further experimentation and production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ NEXT STEPS FOR EXPERIMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. üî¨ Model Architecture Experiments:\")\n",
    "print(\"   - Try different sequence lengths (96, 240, 336 hours)\")\n",
    "print(\"   - Experiment with attention mechanisms\")\n",
    "print(\"   - Test different LSTM hidden sizes (32, 128, 256)\")\n",
    "print(\"   - Add more layers or reduce for simpler models\")\n",
    "\n",
    "print(\"\\n2. üìä Feature Engineering Experiments:\")\n",
    "print(\"   - Add more technical indicators (MACD, Stochastic)\")\n",
    "print(\"   - Include economic calendar features\")\n",
    "print(\"   - Experiment with different volatility windows\")\n",
    "print(\"   - Test cross-currency correlation features\")\n",
    "\n",
    "print(\"\\n3. üìà Data Experiments:\")\n",
    "print(\"   - Use different time periods (2y, 5y historical data)\")\n",
    "print(\"   - Try different intervals (30m, 4h, daily)\")\n",
    "print(\"   - Include more currency pairs\")\n",
    "print(\"   - Add external data (economic indicators, news sentiment)\")\n",
    "\n",
    "print(\"\\n4. üéØ Training Experiments:\")\n",
    "print(\"   - Different loss functions (Huber, Focal Loss)\")\n",
    "print(\"   - Learning rate scheduling strategies\")\n",
    "print(\"   - Different optimizers (AdamW, RMSprop)\")\n",
    "print(\"   - Regularization techniques (weight decay, label smoothing)\")\n",
    "\n",
    "print(\"\\n5. üîÆ Prediction Experiments:\")\n",
    "print(\"   - Multi-horizon predictions (1h, 6h, 24h, 7d)\")\n",
    "print(\"   - Ensemble methods (multiple model voting)\")\n",
    "print(\"   - Online learning and model adaptation\")\n",
    "print(\"   - Backtesting on historical data\")\n",
    "\n",
    "print(\"\\n6. üè≠ Production Integration:\")\n",
    "print(\"   - Model serving with FastAPI\")\n",
    "print(\"   - Real-time data streaming\")\n",
    "print(\"   - Model monitoring and drift detection\")\n",
    "print(\"   - A/B testing framework\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Ready for experimentation! Modify the cells above to test different configurations.\")\n",
    "print(\"\\nüí° Pro tip: Start with the 'quick_experiment' function for rapid testing!\")\n",
    "print(\"\\nüß™ Example experiments to try:\")\n",
    "print(\"   quick_experiment('USD/EUR', sequence_length=96, hidden_size=32, max_epochs=20)\")\n",
    "print(\"   quick_experiment('EUR/GBP', sequence_length=240, hidden_size=128, num_layers=3, max_epochs=40)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}