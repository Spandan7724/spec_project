llm:
  default_provider: "copilot"
  providers:
    copilot:
      model: "gpt-4o-2024-11-20"
      enabled: true
      kwargs:
        temperature: 0.7
        max_tokens: 128000
    copilot_mini:
      model: "gpt-5-mini"
      enabled: true
      kwargs:
        temperature: 0.7
        max_tokens: 128000
    copilot_claude:
      model: "claude-3.5-sonnet"
      enabled: true
      kwargs:
        temperature: 0.7
        max_tokens: 128000
    openai:
      model: "gpt-4"
      enabled: false
      kwargs:
        temperature: 0.7
        max_tokens: 200000
    claude:
      model: "claude-4"
      enabled: false
      kwargs:
        temperature: 0.7
        max_tokens: 200000
  failover:
    enabled: true
    order: ["copilot", "copilot_mini", "openai", "claude"]